{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Weather Load Transform Clean and extract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Investigate world weather data, clean for US and targetted measurements, save out to zip file for use in Dashboard\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* ghcnd yearly weather files and station_list\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* File per year for US and target elements, pivotted to have 1 row per station per day\n",
        "* unified file for US for 2000-2016 limited to stations within 10km of cities from the pollution data.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h:\\VScode\\March Group\\March_Team_Project\\jupyter_notebooks\n",
            "You set a new current directory\n",
            "h:\\VScode\\March Group\\March_Team_Project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "current_dir = os.getcwd()\n",
        "print(current_dir)\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")\n",
        "current_dir = os.getcwd()\n",
        "print(current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load csv files from Not_to_be_shared_to_repo folder\n",
        "# apply the following column names to the file, Station_ID, Date, DataValue, MFlag, QFlag, SFlag, ObsTime\n",
        "weather_df = pd.read_csv('Not_to_be_shared_to_repo/2000.csv.gz', header=None)\n",
        "weather_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
        "ELEMENT = 4 character indicator of element type \n",
        "DATA VALUE = 5 character data value for ELEMENT \n",
        "M-FLAG = 1 character Measurement Flag \n",
        "Q-FLAG = 1 character Quality Flag \n",
        "S-FLAG = 1 character Source Flag \n",
        "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_df.columns = [\"Station_ID\", \"Date\", \"Element\",\"DataValue\", \"MFlag\", \"QFlag\", \"SFlag\", \"ObsTime\"]\n",
        "# set the column names\n",
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load list of Stations from ghcnd=stations.txt, apply column headings and drop unrequired columns then view data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the ghcnd-stations.txt file - fixed width file no header\n",
        "stations_df = pd.read_fwf('Source_Data/ghcnd-stations.txt', header=None)\n",
        "stations_df.columns = [ \"StationId\",\"Latitude\", \"Longitude\", \"Elevation\", \"Name\", \"GSN_Flag\", \"HCN_CRN_Flag\", \"WMO_ID\"]\n",
        "# Drop GSN_Flag, HCN_CRN_Flag, WMO_ID columns\n",
        "stations_df.drop([\"GSN_Flag\", \"HCN_CRN_Flag\", \"WMO_ID\"], axis=1, inplace=True)\n",
        "stations_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load countries data, apply column names, then load states data, apply column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load ghcnd-countries.txt file - fixed width file no header\n",
        "countries_df = pd.read_fwf('Source_Data/ghcnd-countries.txt', header=None)\n",
        "countries_df.columns = [\"CountryCode\", \"Name\"]\n",
        "\n",
        "# Load ghcnd-states.txt file - fixed width file no header\n",
        "states_df = pd.read_fwf('Source_Data/ghcnd-states.txt', header=None)\n",
        "states_df.columns = [\"StateCode\", \"Name\"]\n",
        "\n",
        "print(countries_df.head())\n",
        "print(states_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove country id from beginning of station_id - as explained in GHCND_readme.txt, then drop unrequired columns from data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add column to weather_df for the country code = set it to the first 2 characters of the Station_ID\n",
        "weather_df['CountryCode'] = weather_df['Station_ID'].str[:2]\n",
        "weather_df.head()\n",
        "# drop weather_df MFlag, QFlag, SFlag, ObsTime columns\n",
        "weather_df.drop([\"MFlag\", \"QFlag\", \"SFlag\", \"ObsTime\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate records by Country and deduplicate the Weather data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count of rows in weather_df by country\n",
        "print(weather_df['CountryCode'].value_counts())\n",
        "# deduplicate the weather_df by station_id, element, date, taking the first record if multiple records exist\n",
        "weather_df = weather_df.drop_duplicates(subset=[\"Station_ID\", \"Element\", \"Date\"], keep='first')\n",
        "# count of rows in weather_df by country\n",
        "print(weather_df['CountryCode'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As no duplications identified = this step can be ignored in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From stationid extract the country code and count records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# seperate the countryid out of the stations_df based on 1st 2 chracters of the StationId\n",
        "stations_df['CountryCode'] = stations_df['StationId'].str[:2]\n",
        "# count stations by country\n",
        "print(stations_df['CountryCode'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm that there are no rogue country codes in the stations_df, then filter both files for data from US only , then produce list of Elements in weather data for US records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that country code is only country code\n",
        "# count of rows where CountryCode in stations_df is not in countries_df\n",
        "print(stations_df[~stations_df['CountryCode'].isin(countries_df['CountryCode'])].shape[0])\n",
        "# count of rows in weather_df where CountryCode is not in countries_df\n",
        "print(weather_df[~weather_df['CountryCode'].isin(countries_df['CountryCode'])].shape[0])\n",
        "# count rows in stations_df\n",
        "print(stations_df.shape[0])\n",
        "# count rows in weather_df\n",
        "print(weather_df.shape[0])\n",
        "# filter stations_df to only include rows where CountryCode =\"US\"\n",
        "stations_df = stations_df[stations_df['CountryCode']==\"US\"]\n",
        "# filter weather_df to only include rows where CountryCode =\"US\"\n",
        "weather_df = weather_df[weather_df['CountryCode']==\"US\"]\n",
        "# count rows in stations_df\n",
        "print(stations_df.shape[0])\n",
        "# count rows in weather_df\n",
        "print(weather_df.shape[0])\n",
        "# count of Elements in weather_df\n",
        "print(weather_df['Element'].value_counts())\n",
        "# list of elements with at data in at least 90% of dates and stations\n",
        "elements = weather_df['Element'].value_counts()\n",
        "elements = elements[elements > 0.9 * 365 * 10]\n",
        "elements = elements.index.tolist()\n",
        "print(elements)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter weather for chosen elements, pivot data to produce 1 line per station per day and then perform a comparison of rows containing all elements and those only with the chosen elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ElementList = [\n",
        "  # \"PRCP\",  # Precipitation (tenths of mm)\n",
        "  # \"WT03\",  # Thunder\n",
        "  \"WT16\",  # Rain (may include freezing rain, drizzle, and freezing drizzle)\n",
        "\n",
        "  # \"SNOW\",  # Snowfall (mm)\n",
        "  # \"SNWD\",  # Snow depth (mm)\n",
        "  # \"WESD\",  # Water equivalent of snow on the ground (tenths of mm)\n",
        "  # 'WESF',  # Water equivalent of snowfall (tenths of mm)\n",
        "  \"WT01\",  # Fog, ice fog, or freezing fog (may include heavy fog)\n",
        "  \"WT04\",  # Ice pellets, sleet, snow pellets, or small hail\"\n",
        "  \"WT18\",  # Snow, snow pellets, snow grains, or ice crystals\n",
        "  'WT22',  # Ice fog or freezing fog\n",
        "  \"WT09\",  # Blowing or drifting snow\n",
        "  \n",
        "  \"TMAX\",  # Maximum temperature (tenths of degrees C)\n",
        "  \"TMIN\",  # Minimum temperature (tenths of degrees C)\n",
        "  \"TAVG\",  # Average temperature (tenths of degrees C)\n",
        "\n",
        "  \"TSUN\",  # Daily total sunshine (minutes)\n",
        "\n",
        "  \"AWND\",  # Average daily wind speed (tenths of meters per second)\n",
        "  # \"WDF2\",  # Direction of fastest 2-minute wind (degrees)\n",
        "  # \"WDF5\",  # Direction of fastest 5-second wind (degrees)\n",
        "  # \"WSF2\",  # Fastest 2-minute wind speed (tenths of meters per second)\n",
        "  # \"WSF5\",  # Fastest 5-second wind speed (tenths of meters per second)\n",
        "  # \"FMTM\",  # Time of fastest mile or fastest 1-minute wind (hours and minutes, i.e., HHMM)\n",
        "  \"PGTM\",  # Peak gust time (hours and minutes, i.e., HHMM)\n",
        "  # \"WDMV\",  # 24-hour wind movement (km)\n",
        "  'WT11',  # High or damaging winds\n",
        "  # \"WT06\",  # Glaze or rime\n",
        "  # \"WT08\",  # Smoke or haze\n",
        "  # \"WT05\",  # Hail (may include small hail)\n",
        "  # \"WT02\",  # Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
        "  # \"WT13\",  # Mist\n",
        "  ]\n",
        "\n",
        "# save Stations_df to a csv file called Us_Stations.csv\n",
        "stations_df.to_csv(\"Us_Stations.csv\", index=False)\n",
        "# filter weather_df to only include rows where Element in ElementList\n",
        "weather_df = weather_df[weather_df['Element'].isin(ElementList)]\n",
        "# count rows in weather_df\n",
        "print(weather_df.shape[0])\n",
        "# unpack weather_df Element in to seperate columns with value eq to DataValue where Element = TMAX, grouped by Station_Id and Date\n",
        "weather_up_df = weather_df.pivot_table(index=[\"Station_ID\", \"Date\"],\n",
        "                     columns=\"Element\",\n",
        "                     values=\"DataValue\",\n",
        "                     aggfunc='first').reset_index()\n",
        "\n",
        "# Ensure a column exists for each value in ElementList\n",
        "for element in ElementList:\n",
        "  if element not in weather_up_df.columns:\n",
        "    weather_up_df[element] = None\n",
        "# count rows of weather_df\n",
        "print(weather_up_df.shape[0])\n",
        "# save weather_df to a csv file called Us_Weather_Unpacked.zip\n",
        "weather_up_df.to_csv(\"Us_Weather_Unpacked.zip\", index=False, compression='zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick look at a single station to confirm that the pivot_table is working correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_up_df[weather_up_df[\"Station_ID\"] == \"USW00094173\"].head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do the same for a single element in the original data format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# weather_df.head(100)\n",
        "weather_df[weather_df[\"Element\"] == \"WT01\"].head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Scripts to bulk load and clean yearly data files using techniks from above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load annual data, filter for US and required elements, drop unrequired columns, perform pivot and resave as a zip file, print file name created and row/column counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ElementList = [\n",
        "  \"WT16\",  # Rain (may include freezing rain, drizzle, and freezing drizzle)\n",
        "  \"WT01\",  # Fog, ice fog, or freezing fog (may include heavy fog)\n",
        "  \"WT04\",  # Ice pellets, sleet, snow pellets, or small hail\"\n",
        "  \"WT18\",  # Snow, snow pellets, snow grains, or ice crystals\n",
        "  'WT22',  # Ice fog or freezing fog\n",
        "  \"WT09\",  # Blowing or drifting snow\n",
        "  \"TMAX\",  # Maximum temperature (tenths of degrees C)\n",
        "  \"TMIN\",  # Minimum temperature (tenths of degrees C)\n",
        "  \"TAVG\",  # Average temperature (tenths of degrees C)\n",
        "  \"TSUN\",  # Daily total sunshine (minutes)\n",
        "  \"AWND\",  # Average daily wind speed (tenths of meters per second)\n",
        "  \"PGTM\",  # Peak gust time (hours and minutes, i.e., HHMM)\n",
        "  'WT11',  # High or damaging winds\n",
        "  ]\n",
        "\n",
        "weather_df  = pd.DataFrame()\n",
        "weather_up_df = pd.DataFrame()\n",
        "\n",
        "# list of files to load\n",
        "FileToLoad = [\"2000.csv.gz\",\n",
        "              \"2001.csv.gz\",\n",
        "              \"2002.csv.gz\",\n",
        "              \"2003.csv.gz\",\n",
        "              \"2004.csv.gz\",\n",
        "              \"2005.csv.gz\",\n",
        "              \"2006.csv.gz\",\n",
        "              \"2007.csv.gz\",\n",
        "              \"2008.csv.gz\",\n",
        "              \"2009.csv.gz\",\n",
        "              \"2010.csv.gz\",\n",
        "              \"2011.csv.gz\",\n",
        "              \"2012.csv.gz\",\n",
        "              \"2013.csv.gz\",\n",
        "              \"2014.csv.gz\",\n",
        "              \"2015.csv.gz\",\n",
        "              \"2016.csv.gz\",\n",
        "              ]\n",
        "\n",
        "# load a file from the list above then process it and append it to weather_df\n",
        "for file in FileToLoad:\n",
        "    # add filename to path \"Not_to_be_shared_to_repo/\"\n",
        "    filepath = os.path.join(\"Not_to_be_shared_to_repo\", file)\n",
        "    # setup df\n",
        "    weather_df  = pd.DataFrame()\n",
        "    weather_up_df = pd.DataFrame()\n",
        "    # Print filepath\n",
        "    print(filepath)\n",
        "    # load the file\n",
        "    weather_df = pd.read_csv(filepath, header=None)\n",
        "    # Name columns\n",
        "    weather_df.columns = [\"Station_ID\", \"Date\", \"Element\", \"DataValue\", \"MFlag\", \"QFlag\", \"SFlag\", \"ObsTime\"]\n",
        "    # Extract Country Code\n",
        "    weather_df['CountryCode'] = weather_df['Station_ID'].str[:2]\n",
        "    # Drop unneccesary columns\n",
        "    weather_df.drop([\"MFlag\", \"QFlag\", \"SFlag\", \"ObsTime\"], axis=1, inplace=True)\n",
        "    # Deduplicate weather_df\n",
        "    weather_df = weather_df.drop_duplicates(subset=[\"Station_ID\", \"Element\", \"Date\"], keep='first')\n",
        "    # filter weather_df to only include rows where CountryCode =\"US\"\n",
        "    weather_df = weather_df[weather_df['CountryCode']==\"US\"]\n",
        "    # filter weather_df to only include rows where Element in ElementList\n",
        "    weather_df = weather_df[weather_df['Element'].isin(ElementList)]\n",
        "    # unpack weather_df Element in to seperate columns with value eq to DataValue where Element = TMAX, grouped by Station and Date\n",
        "    weather_df = weather_df.pivot_table(index=[\"Station_ID\", \"Date\"],\n",
        "                                        columns=\"Element\",\n",
        "                                        values=\"DataValue\",\n",
        "                                        aggfunc='first').reset_index()\n",
        "    # print the shape of weather_df\n",
        "    print(weather_df.shape)\n",
        "    # add filename to path \"Not_to_be_shared_to_repo/\"\n",
        "    filename = f\"Us_{file}_Weather_Unpacked.zip\"\n",
        "    filepath = os.path.join(\"Not_to_be_shared_to_repo\", filename)\n",
        "    # implement save append to csv file\n",
        "    weather_df.to_csv(filepath, index=True, mode='a', header=True, compression='zip')\n",
        "    # drop weather_df and weather_up_df\n",
        "    del weather_df\n",
        "    # print file name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load pivotted, filtered data by year and filter for required cities - append into single file and save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Us_2000.csv.gz_Weather_Unpacked.zip\n",
            "(14842, 16)\n",
            "Us_2001.csv.gz_Weather_Unpacked.zip\n",
            "(29554, 16)\n",
            "Us_2002.csv.gz_Weather_Unpacked.zip\n",
            "(43070, 16)\n",
            "Us_2003.csv.gz_Weather_Unpacked.zip\n",
            "(57019, 16)\n",
            "Us_2004.csv.gz_Weather_Unpacked.zip\n",
            "(71274, 16)\n",
            "Us_2005.csv.gz_Weather_Unpacked.zip\n",
            "(85939, 16)\n",
            "Us_2006.csv.gz_Weather_Unpacked.zip\n",
            "(100777, 16)\n",
            "Us_2007.csv.gz_Weather_Unpacked.zip\n",
            "(115989, 16)\n",
            "Us_2008.csv.gz_Weather_Unpacked.zip\n",
            "(131638, 16)\n",
            "Us_2009.csv.gz_Weather_Unpacked.zip\n",
            "(147930, 16)\n",
            "Us_2010.csv.gz_Weather_Unpacked.zip\n",
            "(164269, 16)\n",
            "Us_2011.csv.gz_Weather_Unpacked.zip\n",
            "(181361, 16)\n",
            "Us_2012.csv.gz_Weather_Unpacked.zip\n",
            "(198545, 16)\n",
            "Us_2013.csv.gz_Weather_Unpacked.zip\n",
            "(215154, 16)\n",
            "Us_2014.csv.gz_Weather_Unpacked.zip\n",
            "(231841, 16)\n",
            "Us_2015.csv.gz_Weather_Unpacked.zip\n",
            "(248608, 16)\n",
            "Us_2016.csv.gz_Weather_Unpacked.zip\n",
            "(265147, 16)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "FileToLoad = [\"Us_2000.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2001.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2002.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2003.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2004.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2005.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2006.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2007.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2008.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2009.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2010.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2011.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2012.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2013.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2014.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2015.csv.gz_Weather_Unpacked.zip\",\n",
        "              \"Us_2016.csv.gz_Weather_Unpacked.zip\",\n",
        "              ]\n",
        "\n",
        "weather_df = pd.DataFrame()\n",
        "weather_up_df = pd.DataFrame()\n",
        "# load weather stations within 10Km of pollution data cities\n",
        "stations_df = pd.read_csv(\"Source_Data\\\\Us_Stations_with_City_10km.csv\")\n",
        "\n",
        "\n",
        "# load file from list and append to weather_final_df\n",
        "for file in FileToLoad:\n",
        "    # set path for fileload\n",
        "    loadfile = \"Not_to_be_shared_to_repo/\"\n",
        "    filepath = os.path.join(loadfile, file)\n",
        "    # load file\n",
        "    weather_df = pd.read_csv(filepath)\n",
        "    # filter to only keep stations that are in stations_df\n",
        "    weather_df = weather_df[weather_df['Station_ID'].isin(stations_df['StationId'])]\n",
        "    # append to weather_final_df\n",
        "    weather_up_df = pd.concat([weather_up_df, weather_df], ignore_index=True)\n",
        "    # print file name\n",
        "    print(file)\n",
        "    # print shape of weather_final_df\n",
        "    print(weather_up_df.shape)\n",
        "    # drop weather_df\n",
        "\n",
        "# save weather_final_df to a csv file called Us_Weather_Final.csv\n",
        "weather_up_df.to_csv(\"Not_to_be_shared_to_repo/Us_Weather_Final_10km.zip\", index=True, header=True, compression='zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import csv from source/simplemaps-worldcities-basic.csv\n",
        "cities_df = pd.read_csv(\"Source_Data\\\\simplemaps_uscities_basicv1.90.zip\")\n",
        "# import csv pollution_data_available.csv\n",
        "pollution_df = pd.read_csv(\"pollution_data_available.csv\")\n",
        "# update pollution_df with lat and long from cities_df\n",
        "pollution_df = pollution_df.merge(cities_df, left_on='City', right_on='city', how='left')\n",
        "#show data\n",
        "pollution_df\n",
        "# save pollution_df to a csv file called city_data_inc_latlog_.csv\n",
        "pollution_df.to_csv(\"city_data_inc_latlog_.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Augment the Pollution Dataset with Coordinates:\n",
        "\n",
        "Retrieve the latitude and longitude for each city in the pollution dataset. Resources like the SimpleMaps US Cities Database provide comprehensive data on U.S. cities, including their geographic coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the city_data_inc_latlog_.csv file\n",
        "city_df = pd.read_csv(\"city_data_inc_latlog_.csv\")\n",
        "#drop unneccesary columns\n",
        "# city_df.drop([\"city\", \"admin_name\", \"population\", \"id\", \"zips\", \"County\", \"State\", \"incorporated\", \"source\", \"ranking\", \"timezone\"], axis=1, inplace=True)\n",
        "#show data\n",
        "city_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Distances Between Cities and Stations:\n",
        "\n",
        "Utilize the Haversine formula to compute the great-circle distance between each city and all monitoring stations. This formula calculates the shortest distance over the Earth's surface, providing an accurate measure between two points specified by their latitude and longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# load US_Stations.csv\n",
        "stations_df = pd.read_csv(\"Source_Data\\\\Us_Stations.csv\")\n",
        "#\n",
        "\n",
        "# Function to calculate Haversine distance\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    \n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    \n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    \n",
        "    return R * c  # Distance in km\n",
        "\n",
        "# using the lat and long from the stations_df and city_df calculate the distance between the two\n",
        "stations_df['City'] = None\n",
        "stations_df['CityDistance'] = None\n",
        "for i, station in stations_df.iterrows():\n",
        "    lat1, lon1 = station['Latitude'], station['Longitude']\n",
        "    min_distance = np.inf\n",
        "    for j, city in city_df.iterrows():\n",
        "        lat2, lon2 = city['lat'], city['lng']\n",
        "        distance = haversine(lat1, lon1, lat2, lon2)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            stations_df.at[i, 'City'] = city['city']\n",
        "            stations_df.at[i, 'CityDistance'] = min_distance\n",
        "\n",
        "# display data\n",
        "stations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save stations_df to a csv file called Us_Stations_with_City.csv\n",
        "stations_df.to_csv(\"Source_Data\\\\Us_Stations_with_City.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter stations_df to provide only the stations that are within 10km of a city\n",
        "stations_df = stations_df[stations_df['CityDistance'] < 10]\n",
        "# display data\n",
        "stations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save stations_df to a csv file called Us_Stations_with_City_10km.csv\n",
        "stations_df.to_csv(\"Source_Data\\\\Us_Stations_with_City_10km.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
